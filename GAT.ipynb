{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GAT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyMeCeozwvaMk7AAglbROXuN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/juanferEspinosa/Graph-Analytics/blob/main/GAT.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wBXdQ5D9JvnY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f37e121-14f5-4df2-c717-f7ec8504773f"
      },
      "source": [
        "!pip install -q torch-scatter -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html;\n",
        "!pip install -q torch-sparse -f https://pytorch-geometric.com/whl/torch-1.9.0+cu102.html;\n",
        "!pip install -q torch-geometric;"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[K     |████████████████████████████████| 8.0 MB 5.3 MB/s \n",
            "\u001b[K     |████████████████████████████████| 2.9 MB 4.9 MB/s \n",
            "\u001b[K     |████████████████████████████████| 325 kB 5.1 MB/s \n",
            "\u001b[K     |████████████████████████████████| 407 kB 34.2 MB/s \n",
            "\u001b[K     |████████████████████████████████| 45 kB 3.2 MB/s \n",
            "\u001b[?25h  Building wheel for torch-geometric (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYaSydifMDk_"
      },
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.nn.parameter import Parameter\n",
        "import torch.nn as nn\n",
        "from torch.nn.modules.module import Module\n",
        "\n",
        "import torch.optim as optim\n",
        "from torch.optim.lr_scheduler import MultiStepLR,StepLR\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle as pkl\n",
        "import sys\n",
        "import networkx as nx\n",
        "import scipy.sparse as sp\n",
        "import math\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "from time import perf_counter"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGEI4tInEDcp",
        "outputId": "7ec38bb8-eac0-43da-b352-2d2eacf43472"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "san7073Ieh0c"
      },
      "source": [
        "# Utils functions: visualization\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v8rEbaJtcJgp"
      },
      "source": [
        "def visualize(h, color, epoch=None, loss=None):\n",
        "    plt.figure(figsize=(7,7))\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "\n",
        "    if torch.is_tensor(h):\n",
        "        h = h.detach().cpu().numpy()\n",
        "        plt.scatter(h[:, 0], h[:, 1], s=140, c=color, cmap=\"Set2\")\n",
        "        if epoch is not None and loss is not None:\n",
        "            plt.xlabel(f'Epoch: {epoch}, Loss: {loss.item():.4f}', fontsize=16)\n",
        "    else:\n",
        "        nx.draw_networkx(h, pos=nx.spring_layout(h, seed=42), with_labels=False,\n",
        "                         node_color=color, cmap=\"Set2\")\n",
        "    plt.show()\n",
        "\n",
        "def normalize_adjacency_matrix(A, I):\n",
        "  \"\"\"\n",
        "  Creating a normalized adjacency matrix with self loops.\n",
        "  :param A: Sparse adjacency matrix.\n",
        "  :param I: Identity matrix.\n",
        "  :return A_tile_hat: Normalized adjacency matrix.\"\"\"\n",
        "  \n",
        "  A_tilde = A + I\n",
        "  degrees = A_tilde.sum(axis=0)[0].tolist()\n",
        "  D = sp.diags(degrees, [0])\n",
        "  D = D.power(-0.5)\n",
        "  A_tilde_hat = D.dot(A_tilde).dot(D)\n",
        "  return A_tilde_hat\n",
        "\n",
        "def normalize(mx):\n",
        "  \"\"\"Row-normalize sparse matrix ---> Node features\"\"\"\n",
        "  rowsum = np.array(mx.sum(1))\n",
        "  r_inv = np.power(rowsum, -1).flatten()\n",
        "  r_inv[np.isinf(r_inv)] = 0.\n",
        "  r_mat_inv = sp.diags(r_inv)\n",
        "  mx = r_mat_inv.dot(mx)\n",
        "  return mx\n",
        "\n",
        "def normalizemx(mx):\n",
        "  \"\"\"Normalization for Scattering GCN\"\"\"\n",
        "  degrees = mx.sum(axis=0)[0].tolist()\n",
        "  #    print(degrees)\n",
        "  D = sp.diags(degrees, [0])\n",
        "  D = D.power(-1)\n",
        "  mx = mx.dot(D)\n",
        "  return mx\n",
        "\n",
        "\n",
        "def scattering1st(spmx,order):\n",
        "\n",
        "  I_n = sp.eye(spmx.shape[0])\n",
        "  adj_sct = 0.5*(spmx+I_n) # P = 1/2 * (I + WD^-1)\n",
        "  adj_power = adj_sct\n",
        "  adj_power = sparse_mx_to_torch_sparse_tensor(adj_power).cuda()\n",
        "  adj_sct = sparse_mx_to_torch_sparse_tensor(adj_sct).cuda()\n",
        "  I_n = sparse_mx_to_torch_sparse_tensor(I_n)\n",
        "  if order>1:\n",
        "    for i in range(order-1):\n",
        "      # Generating P^(2^(k-1))\n",
        "      adj_power = torch.spmm(adj_power,adj_sct.to_dense())\n",
        "      print('Generating SCT')\n",
        "    # Generating. final scattering of order K -> (I - P^(2^(k-1))) * P^(2^(k-1))\n",
        "    adj_int = torch.spmm((adj_power-I_n.cuda()),adj_power)\n",
        "  else:\n",
        "    # Generating. final scattering of order K -> (I - P^(2^(k-1))) * P^(2^(k-1))\n",
        "    adj_int = torch.spmm((adj_power-I_n.cuda()),adj_power.to_dense())\n",
        "  return adj_int\n",
        "\n",
        "\n",
        "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
        "  \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
        "  sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
        "  indices = torch.from_numpy(np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
        "  values = torch.from_numpy(sparse_mx.data)\n",
        "  shape = torch.Size(sparse_mx.shape)\n",
        "  return torch.sparse.FloatTensor(indices, values, shape)\n",
        "\n",
        "\n",
        "def parse_index_file(filename):\n",
        "  #Parse index file.\n",
        "  index = []\n",
        "  for line in open(filename):\n",
        "      index.append(int(line.strip()))\n",
        "  return index\n",
        "\n",
        "def accuracy(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels)\n",
        "  correct = preds.eq(labels).double()\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21wp5VSYesK0"
      },
      "source": [
        "# Preprocessing: Importing datasets\n",
        "\n",
        "Importing the datasets, split into training, validation and testing, normalizing it, getting the adjacency matrix, the scattering matrices, features matrix, index of nodes."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XBWtKSG0MNi5"
      },
      "source": [
        "def load_citation(dataset_str=\"cora\", normalization=\"AugNormAdj\", cuda=True):\n",
        "  \"\"\"  \n",
        "  Load Citation Networks Datasets.\n",
        "  \"\"\"\n",
        "  names = ['x', 'y', 'tx', 'ty', 'allx', 'ally', 'graph']\n",
        "  objects = []\n",
        "  for i in range(len(names)):\n",
        "    with open(\"/content/drive/MyDrive/THESIS/Databases/data/ind.{}.{}\".format(dataset_str.lower(), names[i]), 'rb') as f:\n",
        "      if sys.version_info > (3, 0):\n",
        "          objects.append(pkl.load(f, encoding='latin1'))\n",
        "      else:\n",
        "          objects.append(pkl.load(f))\n",
        "\n",
        "  x, y, tx, ty, allx, ally, graph = tuple(objects)\n",
        "  test_idx_reorder = parse_index_file(\"/content/drive/MyDrive/THESIS/Databases/data/ind.{}.test.index\".format(dataset_str))\n",
        "  test_idx_range = np.sort(test_idx_reorder)\n",
        "\n",
        "  if dataset_str == 'citeseer':\n",
        "    # Fix citeseer dataset (there are some isolated nodes in the graph)\n",
        "    # Find isolated nodes, add them as zero-vecs into the right position\n",
        "    test_idx_range_full = range(min(test_idx_reorder), max(test_idx_reorder)+1)\n",
        "    tx_extended = sp.lil_matrix((len(test_idx_range_full), x.shape[1]))\n",
        "    tx_extended[test_idx_range-min(test_idx_range), :] = tx\n",
        "    tx = tx_extended\n",
        "    ty_extended = np.zeros((len(test_idx_range_full), y.shape[1]))\n",
        "    ty_extended[test_idx_range-min(test_idx_range), :] = ty\n",
        "    ty = ty_extended\n",
        "\n",
        "  features = sp.vstack((allx, tx)).tolil()\n",
        "  features[test_idx_reorder, :] = features[test_idx_range, :]\n",
        "  adj = nx.adjacency_matrix(nx.from_dict_of_lists(graph))\n",
        "  adj = adj + adj.T.multiply(adj.T > adj) - adj.multiply(adj.T > adj)\n",
        "  labels = np.vstack((ally, ty))\n",
        "  labels[test_idx_reorder, :] = labels[test_idx_range, :]\n",
        "\n",
        "\n",
        "  idx_test = test_idx_range.tolist()\n",
        "  idx_train = range(len(y))\n",
        "  idx_val = range(len(y), len(y)+500)\n",
        "\n",
        "  #   take from https://github.com/tkipf/pygcn/blob/master/pygcn/utils.py\n",
        "  #    idx_train = range(140)\n",
        "  #    idx_val = range(200, 500)\n",
        "  #    idx_test = range(500, 1500)\n",
        "\n",
        "\n",
        "  labels = torch.LongTensor(labels)\n",
        "  labels = torch.max(labels, dim=1)[1]\n",
        "  idx_train = torch.LongTensor(idx_train)\n",
        "  idx_val = torch.LongTensor(idx_val)\n",
        "  idx_test = torch.LongTensor(idx_test)\n",
        "\n",
        "  features = normalize(features)\n",
        "  A_tilde = normalize_adjacency_matrix(adj,sp.eye(adj.shape[0]))\n",
        "  adj = normalizemx(adj)\n",
        "  features = torch.FloatTensor(np.array(features.todense()))\n",
        "  print('Loading')\n",
        "  #adj_sct1 = scattering1st(adj,1) ## psi_1 = P(I-P)\n",
        "  #adj_sct2 = scattering1st(adj,2) # psi_2 = P^2(I-P^2)\n",
        "  #adj_sct4 = scattering1st(adj,4) # psi_3 = P^4(I-P^4)\n",
        "  adj = sparse_mx_to_torch_sparse_tensor(adj)\n",
        "  A_tilde = sparse_mx_to_torch_sparse_tensor(A_tilde)\n",
        "  return adj,A_tilde,features, labels, idx_train, idx_val, idx_test\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oJmG5IVVEiZ6",
        "outputId": "a9213bd6-f8cb-4f38-8517-4ce24e9665cb"
      },
      "source": [
        "adj,A_tilde,features, labels, idx_train, idx_val, idx_test = load_citation()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PFU-Yff7Ziw"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hS7t580peyK8"
      },
      "source": [
        "# MODELS\n",
        "\n",
        "First the convolutional structure is defined to finally being called in a nn Module. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZAabB4Oy0u9W"
      },
      "source": [
        "class GraphAttentionLayer(nn.Module):\n",
        "  \"\"\"\n",
        "  Simple GAT layer, similar to https://arxiv.org/abs/1710.10903\n",
        "  \"\"\"\n",
        "  def __init__(self, in_features, out_features, dropout, alpha, concat=True):\n",
        "    super(GraphAttentionLayer, self).__init__()\n",
        "    self.dropout = dropout\n",
        "    self.in_features = in_features\n",
        "    self.out_features = out_features\n",
        "    self.alpha = alpha\n",
        "    self.concat = concat\n",
        "\n",
        "    self.W = nn.Parameter(torch.empty(size=(in_features, out_features)))\n",
        "    nn.init.xavier_uniform_(self.W.data, gain=1.414)\n",
        "    self.a = nn.Parameter(torch.empty(size=(2*out_features, 1)))\n",
        "    nn.init.xavier_uniform_(self.a.data, gain=1.414)\n",
        "\n",
        "    self.leakyrelu = nn.LeakyReLU(self.alpha)\n",
        "\n",
        "  def forward(self, h, adj):\n",
        "    Wh = torch.mm(h, self.W) # h.shape: (N, in_features), Wh.shape: (N, out_features)\n",
        "    e = self._prepare_attentional_mechanism_input(Wh)\n",
        "    zero_vec = -9e15*torch.ones_like(e)\n",
        "    attention = torch.where(adj.to_dense() > 0, e, zero_vec)\n",
        "    attention = F.softmax(attention, dim=1)\n",
        "    attention = F.dropout(attention, self.dropout, training=self.training)\n",
        "    h_prime = torch.matmul(attention, Wh)\n",
        "    if self.concat:\n",
        "        return F.elu(h_prime)\n",
        "    else:\n",
        "        return h_prime\n",
        "\n",
        "  def _prepare_attentional_mechanism_input(self, Wh):\n",
        "    # Wh.shape (N, out_feature)\n",
        "    # self.a.shape (2 * out_feature, 1)\n",
        "    # Wh1&2.shape (N, 1)\n",
        "    # e.shape (N, N)\n",
        "    Wh1 = torch.matmul(Wh, self.a[:self.out_features, :])\n",
        "    Wh2 = torch.matmul(Wh, self.a[self.out_features:, :])\n",
        "    # broadcast add\n",
        "    e = Wh1 + Wh2.T\n",
        "    return self.leakyrelu(e)\n",
        "\n",
        "  def __repr__(self):\n",
        "    return self.__class__.__name__ + ' (' + str(self.in_features) + ' -> ' + str(self.out_features) + ')'"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LvI__GAEMvlS"
      },
      "source": [
        "class GAT(nn.Module):\n",
        "  def __init__(self, nfeat, nhid, nclass, dropout, alpha, nheads):\n",
        "    \"\"\"Dense version of GAT.\"\"\"\n",
        "    super(GAT, self).__init__()\n",
        "    self.dropout = dropout\n",
        "\n",
        "    self.attentions = [GraphAttentionLayer(nfeat, nhid, dropout=dropout, alpha=alpha, concat=True) for _ in range(nheads)]\n",
        "    for i, attention in enumerate(self.attentions):\n",
        "        self.add_module('attention_{}'.format(i), attention)\n",
        "\n",
        "    self.out_att = GraphAttentionLayer(nhid * nheads, nclass, dropout=dropout, alpha=alpha, concat=False)\n",
        "\n",
        "  def forward(self, x, adj):\n",
        "    x = F.dropout(x, self.dropout, training=self.training)\n",
        "    # [x,y] concatenation\n",
        "    x = torch.cat([att(x, adj) for att in self.attentions], dim=1)\n",
        "    x = F.dropout(x, self.dropout, training=self.training)\n",
        "    x = F.elu(self.out_att(x, adj))\n",
        "    return F.log_softmax(x, dim=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_brRGa0pe40k"
      },
      "source": [
        "# Execution of the overall model\n",
        "\n",
        "Hyperparameter definition, model instatiated, and training and testing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gzm8QWtOM7cH"
      },
      "source": [
        "from torch.autograd import Variable\n",
        "torch.manual_seed(42)\n",
        "\n",
        "\n",
        "\n",
        "epochs = 200\n",
        "lr = 0.01\n",
        "patience = 100\n",
        "fastmode =False\n",
        "cuda = torch.cuda.is_available()\n",
        "model = GAT(nfeat=features.shape[1], \n",
        "                nhid=8, \n",
        "                nclass=int(labels.max()) + 1, \n",
        "                dropout=0.6, \n",
        "                nheads=8, \n",
        "                alpha=0.2)\n",
        "\n",
        "if cuda:\n",
        "    model = model.cuda()\n",
        "    features = features.cuda()\n",
        "    A_tilde = A_tilde.cuda()\n",
        "    adj = adj.cuda()\n",
        "    labels = labels.cuda()\n",
        "    idx_train = idx_train.cuda()\n",
        "    idx_val = idx_val.cuda()\n",
        "    idx_test = idx_test.cuda()\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),lr=lr)\n",
        "criterion = torch.nn.CrossEntropyLoss()\n",
        "scheduler = StepLR(optimizer, step_size=50, gamma=0.9)\n",
        "\n",
        "features, adj, labels = Variable(features), Variable(adj), Variable(labels)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 430
        },
        "id": "DgD1SpIcNhUT",
        "outputId": "4adc9f2f-8ed8-4042-bca3-4e18db7ade16"
      },
      "source": [
        "\n",
        "import time\n",
        "import glob\n",
        "import os\n",
        "from IPython.display import Javascript  # Restrict height of output cell.\n",
        "display(Javascript('''google.colab.output.setIframeHeight(0, true, {maxHeight: 430})'''))\n",
        "\n",
        "def acc1(output, labels):\n",
        "  preds = output.max(1)[1].type_as(labels)\n",
        "  correct = preds.eq(labels).double()\n",
        "  correct = correct.sum()\n",
        "  return correct / len(labels)\n",
        "def train(epoch):\n",
        "  t = time.time()\n",
        "  model.train()\n",
        "  optimizer.zero_grad()\n",
        "  output = model(features, A_tilde)\n",
        "  loss_train = F.nll_loss(output[idx_train], labels[idx_train])\n",
        "  acc_train = accuracy(output[idx_train], labels[idx_train])\n",
        "  loss_train.backward()\n",
        "  optimizer.step()\n",
        "\n",
        "  if not fastmode:\n",
        "    # Evaluate validation set performance separately,\n",
        "    # deactivates dropout during validation run.\n",
        "    model.eval()\n",
        "    output = model(features, A_tilde)\n",
        "\n",
        "  loss_val = F.nll_loss(output[idx_val], labels[idx_val])\n",
        "  acc_val = accuracy(output[idx_val], labels[idx_val])\n",
        "  print('Epoch: {:04d}'.format(epoch+1),\n",
        "        'loss_train: {:.4f}'.format(loss_train.data.item()),\n",
        "        'acc_train: {:.4f}'.format(acc_train.data.item()),\n",
        "        'loss_val: {:.4f}'.format(loss_val.data.item()),\n",
        "        'acc_val: {:.4f}'.format(acc_val.data.item()),\n",
        "        'time: {:.4f}s'.format(time.time() - t))\n",
        "\n",
        "  return loss_val.data.item()\n",
        "\n",
        "\n",
        "def compute_test():\n",
        "  model.eval()\n",
        "  output = model(features, A_tilde)\n",
        "  loss_test = F.nll_loss(output[idx_test], labels[idx_test])\n",
        "  acc_test = accuracy(output[idx_test], labels[idx_test])\n",
        "  print(\"Test set results:\",\n",
        "        \"loss= {:.4f}\".format(loss_test.data.item()),\n",
        "        \"accuracy= {:.4f}\".format(acc_test.data.item()))\n",
        "\n",
        "# Train model\n",
        "t_total = time.time()\n",
        "loss_values = []\n",
        "bad_counter = 0\n",
        "best = epochs + 1\n",
        "best_epoch = 0\n",
        "for epoch in range(epochs):\n",
        "  loss_values.append(train(epoch))\n",
        "\n",
        "  torch.save(model.state_dict(), '{}.pkl'.format(epoch))\n",
        "  if loss_values[-1] < best:\n",
        "    best = loss_values[-1]\n",
        "    best_epoch = epoch\n",
        "    bad_counter = 0\n",
        "  else:\n",
        "    bad_counter += 1\n",
        "\n",
        "  if bad_counter == patience:\n",
        "    break\n",
        "\n",
        "  files = glob.glob('*.pkl')\n",
        "  for file in files:\n",
        "    epoch_nb = int(file.split('.')[0])\n",
        "    if epoch_nb < best_epoch:\n",
        "        os.remove(file)\n",
        "\n",
        "files = glob.glob('*.pkl')\n",
        "for file in files:\n",
        "  epoch_nb = int(file.split('.')[0])\n",
        "  if epoch_nb > best_epoch:\n",
        "      os.remove(file)\n",
        "\n",
        "print(\"Optimization Finished!\")\n",
        "print(\"Total time elapsed: {:.4f}s\".format(time.time() - t_total))\n",
        "\n",
        "# Restore best model\n",
        "print('Loading {}th epoch'.format(best_epoch))\n",
        "model.load_state_dict(torch.load('{}.pkl'.format(best_epoch)))\n",
        "\n",
        "# Testing\n",
        "compute_test()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "application/javascript": [
              "google.colab.output.setIframeHeight(0, true, {maxHeight: 430})"
            ],
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 0001 loss_train: 1.9529 acc_train: 0.0929 loss_val: 1.9329 acc_val: 0.4780 time: 0.1739s\n",
            "Epoch: 0002 loss_train: 1.9305 acc_train: 0.2643 loss_val: 1.9197 acc_val: 0.6160 time: 0.1553s\n",
            "Epoch: 0003 loss_train: 1.9029 acc_train: 0.4857 loss_val: 1.9068 acc_val: 0.6720 time: 0.1460s\n",
            "Epoch: 0004 loss_train: 1.8832 acc_train: 0.5929 loss_val: 1.8936 acc_val: 0.6860 time: 0.1384s\n",
            "Epoch: 0005 loss_train: 1.8666 acc_train: 0.5643 loss_val: 1.8801 acc_val: 0.7200 time: 0.1368s\n",
            "Epoch: 0006 loss_train: 1.8408 acc_train: 0.6357 loss_val: 1.8663 acc_val: 0.7360 time: 0.1359s\n",
            "Epoch: 0007 loss_train: 1.8209 acc_train: 0.6786 loss_val: 1.8521 acc_val: 0.7380 time: 0.1311s\n",
            "Epoch: 0008 loss_train: 1.8008 acc_train: 0.6500 loss_val: 1.8376 acc_val: 0.7400 time: 0.1310s\n",
            "Epoch: 0009 loss_train: 1.7905 acc_train: 0.6571 loss_val: 1.8223 acc_val: 0.7380 time: 0.1310s\n",
            "Epoch: 0010 loss_train: 1.7358 acc_train: 0.7000 loss_val: 1.8058 acc_val: 0.7360 time: 0.1308s\n",
            "Epoch: 0011 loss_train: 1.7367 acc_train: 0.6929 loss_val: 1.7887 acc_val: 0.7400 time: 0.1312s\n",
            "Epoch: 0012 loss_train: 1.6906 acc_train: 0.7857 loss_val: 1.7707 acc_val: 0.7480 time: 0.1311s\n",
            "Epoch: 0013 loss_train: 1.6910 acc_train: 0.7000 loss_val: 1.7521 acc_val: 0.7500 time: 0.1313s\n",
            "Epoch: 0014 loss_train: 1.6266 acc_train: 0.7500 loss_val: 1.7328 acc_val: 0.7520 time: 0.1313s\n",
            "Epoch: 0015 loss_train: 1.6087 acc_train: 0.7214 loss_val: 1.7128 acc_val: 0.7540 time: 0.1312s\n",
            "Epoch: 0016 loss_train: 1.5601 acc_train: 0.7786 loss_val: 1.6924 acc_val: 0.7580 time: 0.1335s\n",
            "Epoch: 0017 loss_train: 1.5541 acc_train: 0.7429 loss_val: 1.6712 acc_val: 0.7560 time: 0.1315s\n",
            "Epoch: 0018 loss_train: 1.5546 acc_train: 0.7000 loss_val: 1.6493 acc_val: 0.7560 time: 0.1310s\n",
            "Epoch: 0019 loss_train: 1.5036 acc_train: 0.7786 loss_val: 1.6270 acc_val: 0.7600 time: 0.1313s\n",
            "Epoch: 0020 loss_train: 1.5178 acc_train: 0.7071 loss_val: 1.6042 acc_val: 0.7600 time: 0.1312s\n",
            "Epoch: 0021 loss_train: 1.4549 acc_train: 0.7786 loss_val: 1.5804 acc_val: 0.7640 time: 0.1314s\n",
            "Epoch: 0022 loss_train: 1.4271 acc_train: 0.7643 loss_val: 1.5556 acc_val: 0.7740 time: 0.1312s\n",
            "Epoch: 0023 loss_train: 1.3505 acc_train: 0.7714 loss_val: 1.5301 acc_val: 0.7840 time: 0.1304s\n",
            "Epoch: 0024 loss_train: 1.3001 acc_train: 0.8429 loss_val: 1.5042 acc_val: 0.7860 time: 0.1310s\n",
            "Epoch: 0025 loss_train: 1.3471 acc_train: 0.8071 loss_val: 1.4782 acc_val: 0.7880 time: 0.1316s\n",
            "Epoch: 0026 loss_train: 1.3135 acc_train: 0.7857 loss_val: 1.4522 acc_val: 0.7860 time: 0.1310s\n",
            "Epoch: 0027 loss_train: 1.2410 acc_train: 0.7929 loss_val: 1.4264 acc_val: 0.7840 time: 0.1308s\n",
            "Epoch: 0028 loss_train: 1.2685 acc_train: 0.7429 loss_val: 1.4009 acc_val: 0.7800 time: 0.1313s\n",
            "Epoch: 0029 loss_train: 1.1727 acc_train: 0.8000 loss_val: 1.3759 acc_val: 0.7820 time: 0.1310s\n",
            "Epoch: 0030 loss_train: 1.1426 acc_train: 0.8429 loss_val: 1.3510 acc_val: 0.7860 time: 0.1309s\n",
            "Epoch: 0031 loss_train: 1.1436 acc_train: 0.8071 loss_val: 1.3264 acc_val: 0.7860 time: 0.1311s\n",
            "Epoch: 0032 loss_train: 1.1263 acc_train: 0.8429 loss_val: 1.3024 acc_val: 0.7860 time: 0.1310s\n",
            "Epoch: 0033 loss_train: 1.0308 acc_train: 0.8143 loss_val: 1.2785 acc_val: 0.7860 time: 0.1307s\n",
            "Epoch: 0034 loss_train: 1.1109 acc_train: 0.8214 loss_val: 1.2551 acc_val: 0.7840 time: 0.1320s\n",
            "Epoch: 0035 loss_train: 0.9843 acc_train: 0.8286 loss_val: 1.2319 acc_val: 0.7820 time: 0.1308s\n",
            "Epoch: 0036 loss_train: 1.0277 acc_train: 0.8000 loss_val: 1.2092 acc_val: 0.7840 time: 0.1310s\n",
            "Epoch: 0037 loss_train: 1.0485 acc_train: 0.7857 loss_val: 1.1870 acc_val: 0.7860 time: 0.1311s\n",
            "Epoch: 0038 loss_train: 1.0625 acc_train: 0.8071 loss_val: 1.1655 acc_val: 0.7880 time: 0.1311s\n",
            "Epoch: 0039 loss_train: 0.9529 acc_train: 0.8071 loss_val: 1.1442 acc_val: 0.7880 time: 0.1336s\n",
            "Epoch: 0040 loss_train: 0.8940 acc_train: 0.8214 loss_val: 1.1241 acc_val: 0.7940 time: 0.1313s\n",
            "Epoch: 0041 loss_train: 0.9818 acc_train: 0.7643 loss_val: 1.1044 acc_val: 0.7920 time: 0.1311s\n",
            "Epoch: 0042 loss_train: 0.9351 acc_train: 0.8071 loss_val: 1.0855 acc_val: 0.7900 time: 0.1310s\n",
            "Epoch: 0043 loss_train: 0.9294 acc_train: 0.8071 loss_val: 1.0675 acc_val: 0.7880 time: 0.1311s\n",
            "Epoch: 0044 loss_train: 0.9636 acc_train: 0.7786 loss_val: 1.0497 acc_val: 0.7860 time: 0.1307s\n",
            "Epoch: 0045 loss_train: 0.8666 acc_train: 0.8429 loss_val: 1.0322 acc_val: 0.7860 time: 0.1311s\n",
            "Epoch: 0046 loss_train: 0.9008 acc_train: 0.7857 loss_val: 1.0153 acc_val: 0.7880 time: 0.1313s\n",
            "Epoch: 0047 loss_train: 0.8340 acc_train: 0.8429 loss_val: 0.9991 acc_val: 0.7900 time: 0.1312s\n",
            "Epoch: 0048 loss_train: 0.8413 acc_train: 0.8357 loss_val: 0.9838 acc_val: 0.7900 time: 0.1313s\n",
            "Epoch: 0049 loss_train: 0.7098 acc_train: 0.8714 loss_val: 0.9690 acc_val: 0.7880 time: 0.1309s\n",
            "Epoch: 0050 loss_train: 0.8097 acc_train: 0.8000 loss_val: 0.9550 acc_val: 0.7900 time: 0.1313s\n",
            "Epoch: 0051 loss_train: 0.8130 acc_train: 0.8429 loss_val: 0.9420 acc_val: 0.7900 time: 0.1314s\n",
            "Epoch: 0052 loss_train: 0.8317 acc_train: 0.8143 loss_val: 0.9298 acc_val: 0.7900 time: 0.1312s\n",
            "Epoch: 0053 loss_train: 0.8219 acc_train: 0.8500 loss_val: 0.9181 acc_val: 0.7920 time: 0.1309s\n",
            "Epoch: 0054 loss_train: 0.8598 acc_train: 0.7714 loss_val: 0.9071 acc_val: 0.7920 time: 0.1310s\n",
            "Epoch: 0055 loss_train: 0.7441 acc_train: 0.8071 loss_val: 0.8965 acc_val: 0.7880 time: 0.1311s\n",
            "Epoch: 0056 loss_train: 0.7616 acc_train: 0.8143 loss_val: 0.8868 acc_val: 0.7880 time: 0.1312s\n",
            "Epoch: 0057 loss_train: 0.7660 acc_train: 0.8286 loss_val: 0.8775 acc_val: 0.7880 time: 0.1311s\n",
            "Epoch: 0058 loss_train: 0.6284 acc_train: 0.8929 loss_val: 0.8691 acc_val: 0.7880 time: 0.1315s\n",
            "Epoch: 0059 loss_train: 0.6537 acc_train: 0.8429 loss_val: 0.8607 acc_val: 0.7880 time: 0.1315s\n",
            "Epoch: 0060 loss_train: 0.6917 acc_train: 0.8429 loss_val: 0.8527 acc_val: 0.7900 time: 0.1318s\n",
            "Epoch: 0061 loss_train: 0.7734 acc_train: 0.7929 loss_val: 0.8449 acc_val: 0.7900 time: 0.1312s\n",
            "Epoch: 0062 loss_train: 0.6696 acc_train: 0.8571 loss_val: 0.8378 acc_val: 0.7900 time: 0.1309s\n",
            "Epoch: 0063 loss_train: 0.6539 acc_train: 0.8429 loss_val: 0.8309 acc_val: 0.7920 time: 0.1309s\n",
            "Epoch: 0064 loss_train: 0.6260 acc_train: 0.8071 loss_val: 0.8239 acc_val: 0.7920 time: 0.1311s\n",
            "Epoch: 0065 loss_train: 0.7869 acc_train: 0.7786 loss_val: 0.8171 acc_val: 0.7940 time: 0.1312s\n",
            "Epoch: 0066 loss_train: 0.7309 acc_train: 0.7857 loss_val: 0.8106 acc_val: 0.7940 time: 0.1313s\n",
            "Epoch: 0067 loss_train: 0.7309 acc_train: 0.8000 loss_val: 0.8046 acc_val: 0.7940 time: 0.1310s\n",
            "Epoch: 0068 loss_train: 0.6443 acc_train: 0.8714 loss_val: 0.7987 acc_val: 0.7960 time: 0.1309s\n",
            "Epoch: 0069 loss_train: 0.7539 acc_train: 0.7929 loss_val: 0.7933 acc_val: 0.7980 time: 0.1307s\n",
            "Epoch: 0070 loss_train: 0.6084 acc_train: 0.8500 loss_val: 0.7881 acc_val: 0.8000 time: 0.1310s\n",
            "Epoch: 0071 loss_train: 0.6745 acc_train: 0.8143 loss_val: 0.7828 acc_val: 0.8020 time: 0.1310s\n",
            "Epoch: 0072 loss_train: 0.6329 acc_train: 0.8143 loss_val: 0.7781 acc_val: 0.8020 time: 0.1309s\n",
            "Epoch: 0073 loss_train: 0.7132 acc_train: 0.8357 loss_val: 0.7727 acc_val: 0.8040 time: 0.1307s\n",
            "Epoch: 0074 loss_train: 0.6455 acc_train: 0.8357 loss_val: 0.7676 acc_val: 0.8040 time: 0.1312s\n",
            "Epoch: 0075 loss_train: 0.6055 acc_train: 0.8214 loss_val: 0.7627 acc_val: 0.8040 time: 0.1307s\n",
            "Epoch: 0076 loss_train: 0.5823 acc_train: 0.8500 loss_val: 0.7579 acc_val: 0.8040 time: 0.1312s\n",
            "Epoch: 0077 loss_train: 0.6249 acc_train: 0.8429 loss_val: 0.7529 acc_val: 0.8020 time: 0.1312s\n",
            "Epoch: 0078 loss_train: 0.6437 acc_train: 0.8500 loss_val: 0.7475 acc_val: 0.8020 time: 0.1311s\n",
            "Epoch: 0079 loss_train: 0.5621 acc_train: 0.8643 loss_val: 0.7423 acc_val: 0.8040 time: 0.1314s\n",
            "Epoch: 0080 loss_train: 0.5911 acc_train: 0.8286 loss_val: 0.7371 acc_val: 0.8040 time: 0.1313s\n",
            "Epoch: 0081 loss_train: 0.5198 acc_train: 0.8214 loss_val: 0.7320 acc_val: 0.8020 time: 0.1310s\n",
            "Epoch: 0082 loss_train: 0.6688 acc_train: 0.8071 loss_val: 0.7274 acc_val: 0.8020 time: 0.1311s\n",
            "Epoch: 0083 loss_train: 0.4883 acc_train: 0.8857 loss_val: 0.7235 acc_val: 0.8040 time: 0.1307s\n",
            "Epoch: 0084 loss_train: 0.6015 acc_train: 0.8071 loss_val: 0.7198 acc_val: 0.8040 time: 0.1305s\n",
            "Epoch: 0085 loss_train: 0.6039 acc_train: 0.8357 loss_val: 0.7164 acc_val: 0.8060 time: 0.1309s\n",
            "Epoch: 0086 loss_train: 0.5575 acc_train: 0.8429 loss_val: 0.7134 acc_val: 0.8080 time: 0.1309s\n",
            "Epoch: 0087 loss_train: 0.6196 acc_train: 0.8214 loss_val: 0.7102 acc_val: 0.8100 time: 0.1311s\n",
            "Epoch: 0088 loss_train: 0.5559 acc_train: 0.8429 loss_val: 0.7074 acc_val: 0.8100 time: 0.1308s\n",
            "Epoch: 0089 loss_train: 0.5270 acc_train: 0.8500 loss_val: 0.7051 acc_val: 0.8080 time: 0.1313s\n",
            "Epoch: 0090 loss_train: 0.6209 acc_train: 0.8214 loss_val: 0.7030 acc_val: 0.8060 time: 0.1308s\n",
            "Epoch: 0091 loss_train: 0.5847 acc_train: 0.8571 loss_val: 0.7009 acc_val: 0.8040 time: 0.1309s\n",
            "Epoch: 0092 loss_train: 0.6611 acc_train: 0.8214 loss_val: 0.6992 acc_val: 0.8020 time: 0.1310s\n",
            "Epoch: 0093 loss_train: 0.5940 acc_train: 0.7857 loss_val: 0.6978 acc_val: 0.8040 time: 0.1308s\n",
            "Epoch: 0094 loss_train: 0.5112 acc_train: 0.8929 loss_val: 0.6963 acc_val: 0.8040 time: 0.1316s\n",
            "Epoch: 0095 loss_train: 0.5909 acc_train: 0.8000 loss_val: 0.6948 acc_val: 0.8020 time: 0.1311s\n",
            "Epoch: 0096 loss_train: 0.4317 acc_train: 0.9071 loss_val: 0.6933 acc_val: 0.8020 time: 0.1312s\n",
            "Epoch: 0097 loss_train: 0.5020 acc_train: 0.8714 loss_val: 0.6919 acc_val: 0.8020 time: 0.1313s\n",
            "Epoch: 0098 loss_train: 0.5884 acc_train: 0.7929 loss_val: 0.6911 acc_val: 0.8020 time: 0.1316s\n",
            "Epoch: 0099 loss_train: 0.5376 acc_train: 0.8214 loss_val: 0.6903 acc_val: 0.8020 time: 0.1312s\n",
            "Epoch: 0100 loss_train: 0.5606 acc_train: 0.8571 loss_val: 0.6903 acc_val: 0.8020 time: 0.1308s\n",
            "Epoch: 0101 loss_train: 0.4666 acc_train: 0.8571 loss_val: 0.6902 acc_val: 0.8000 time: 0.1309s\n",
            "Epoch: 0102 loss_train: 0.5065 acc_train: 0.8357 loss_val: 0.6900 acc_val: 0.8000 time: 0.1310s\n",
            "Epoch: 0103 loss_train: 0.4403 acc_train: 0.8286 loss_val: 0.6897 acc_val: 0.7980 time: 0.1308s\n",
            "Epoch: 0104 loss_train: 0.5082 acc_train: 0.8071 loss_val: 0.6896 acc_val: 0.7980 time: 0.1338s\n",
            "Epoch: 0105 loss_train: 0.4665 acc_train: 0.8786 loss_val: 0.6892 acc_val: 0.7980 time: 0.1312s\n",
            "Epoch: 0106 loss_train: 0.4676 acc_train: 0.8643 loss_val: 0.6888 acc_val: 0.7980 time: 0.1315s\n",
            "Epoch: 0107 loss_train: 0.3910 acc_train: 0.9000 loss_val: 0.6887 acc_val: 0.7980 time: 0.1308s\n",
            "Epoch: 0108 loss_train: 0.4706 acc_train: 0.8429 loss_val: 0.6893 acc_val: 0.7960 time: 0.1314s\n",
            "Epoch: 0109 loss_train: 0.5286 acc_train: 0.8143 loss_val: 0.6897 acc_val: 0.7960 time: 0.1311s\n",
            "Epoch: 0110 loss_train: 0.4291 acc_train: 0.8571 loss_val: 0.6897 acc_val: 0.7960 time: 0.1309s\n",
            "Epoch: 0111 loss_train: 0.5345 acc_train: 0.8214 loss_val: 0.6897 acc_val: 0.7960 time: 0.1309s\n",
            "Epoch: 0112 loss_train: 0.4217 acc_train: 0.8571 loss_val: 0.6891 acc_val: 0.7940 time: 0.1312s\n",
            "Epoch: 0113 loss_train: 0.5260 acc_train: 0.8357 loss_val: 0.6886 acc_val: 0.7940 time: 0.1310s\n",
            "Epoch: 0114 loss_train: 0.5076 acc_train: 0.8214 loss_val: 0.6880 acc_val: 0.7940 time: 0.1311s\n",
            "Epoch: 0115 loss_train: 0.4736 acc_train: 0.8571 loss_val: 0.6870 acc_val: 0.7960 time: 0.1309s\n",
            "Epoch: 0116 loss_train: 0.4371 acc_train: 0.8500 loss_val: 0.6853 acc_val: 0.7960 time: 0.1313s\n",
            "Epoch: 0117 loss_train: 0.5281 acc_train: 0.8143 loss_val: 0.6836 acc_val: 0.7960 time: 0.1313s\n",
            "Epoch: 0118 loss_train: 0.4253 acc_train: 0.9000 loss_val: 0.6821 acc_val: 0.7960 time: 0.1310s\n",
            "Epoch: 0119 loss_train: 0.4756 acc_train: 0.8786 loss_val: 0.6809 acc_val: 0.7980 time: 0.1315s\n",
            "Epoch: 0120 loss_train: 0.5729 acc_train: 0.8286 loss_val: 0.6799 acc_val: 0.8000 time: 0.1317s\n",
            "Epoch: 0121 loss_train: 0.5594 acc_train: 0.8214 loss_val: 0.6788 acc_val: 0.8040 time: 0.1315s\n",
            "Epoch: 0122 loss_train: 0.4145 acc_train: 0.8786 loss_val: 0.6772 acc_val: 0.8040 time: 0.1311s\n",
            "Epoch: 0123 loss_train: 0.4578 acc_train: 0.8714 loss_val: 0.6752 acc_val: 0.8060 time: 0.1313s\n",
            "Epoch: 0124 loss_train: 0.4042 acc_train: 0.9000 loss_val: 0.6736 acc_val: 0.8060 time: 0.1313s\n",
            "Epoch: 0125 loss_train: 0.4293 acc_train: 0.8786 loss_val: 0.6719 acc_val: 0.8060 time: 0.1314s\n",
            "Epoch: 0126 loss_train: 0.4183 acc_train: 0.8571 loss_val: 0.6709 acc_val: 0.8060 time: 0.1310s\n",
            "Epoch: 0127 loss_train: 0.4086 acc_train: 0.8571 loss_val: 0.6697 acc_val: 0.8060 time: 0.1315s\n",
            "Epoch: 0128 loss_train: 0.4984 acc_train: 0.8500 loss_val: 0.6684 acc_val: 0.8040 time: 0.1313s\n",
            "Epoch: 0129 loss_train: 0.4587 acc_train: 0.8571 loss_val: 0.6671 acc_val: 0.8020 time: 0.1316s\n",
            "Epoch: 0130 loss_train: 0.5028 acc_train: 0.8286 loss_val: 0.6662 acc_val: 0.8020 time: 0.1317s\n",
            "Epoch: 0131 loss_train: 0.4813 acc_train: 0.8643 loss_val: 0.6657 acc_val: 0.8020 time: 0.1313s\n",
            "Epoch: 0132 loss_train: 0.4067 acc_train: 0.8929 loss_val: 0.6655 acc_val: 0.8020 time: 0.1311s\n",
            "Epoch: 0133 loss_train: 0.4071 acc_train: 0.8857 loss_val: 0.6650 acc_val: 0.8020 time: 0.1312s\n",
            "Epoch: 0134 loss_train: 0.3503 acc_train: 0.8929 loss_val: 0.6647 acc_val: 0.8040 time: 0.1311s\n",
            "Epoch: 0135 loss_train: 0.4300 acc_train: 0.8429 loss_val: 0.6644 acc_val: 0.8040 time: 0.1317s\n",
            "Epoch: 0136 loss_train: 0.4725 acc_train: 0.8357 loss_val: 0.6641 acc_val: 0.8040 time: 0.1310s\n",
            "Epoch: 0137 loss_train: 0.4626 acc_train: 0.8500 loss_val: 0.6640 acc_val: 0.8060 time: 0.1311s\n",
            "Epoch: 0138 loss_train: 0.3918 acc_train: 0.8643 loss_val: 0.6646 acc_val: 0.8060 time: 0.1314s\n",
            "Epoch: 0139 loss_train: 0.4631 acc_train: 0.8500 loss_val: 0.6651 acc_val: 0.8060 time: 0.1309s\n",
            "Epoch: 0140 loss_train: 0.3805 acc_train: 0.9214 loss_val: 0.6656 acc_val: 0.8060 time: 0.1310s\n",
            "Epoch: 0141 loss_train: 0.3317 acc_train: 0.9000 loss_val: 0.6662 acc_val: 0.8040 time: 0.1308s\n",
            "Epoch: 0142 loss_train: 0.3990 acc_train: 0.8643 loss_val: 0.6670 acc_val: 0.8020 time: 0.1316s\n",
            "Epoch: 0143 loss_train: 0.4619 acc_train: 0.8500 loss_val: 0.6675 acc_val: 0.8040 time: 0.1310s\n",
            "Epoch: 0144 loss_train: 0.3926 acc_train: 0.8929 loss_val: 0.6677 acc_val: 0.8040 time: 0.1307s\n",
            "Epoch: 0145 loss_train: 0.4149 acc_train: 0.8500 loss_val: 0.6682 acc_val: 0.8060 time: 0.1307s\n",
            "Epoch: 0146 loss_train: 0.3697 acc_train: 0.8929 loss_val: 0.6689 acc_val: 0.8020 time: 0.1309s\n",
            "Epoch: 0147 loss_train: 0.3651 acc_train: 0.8571 loss_val: 0.6694 acc_val: 0.8020 time: 0.1310s\n",
            "Epoch: 0148 loss_train: 0.3540 acc_train: 0.8857 loss_val: 0.6697 acc_val: 0.8000 time: 0.1306s\n",
            "Epoch: 0149 loss_train: 0.3785 acc_train: 0.8643 loss_val: 0.6704 acc_val: 0.8000 time: 0.1315s\n",
            "Epoch: 0150 loss_train: 0.3996 acc_train: 0.8714 loss_val: 0.6710 acc_val: 0.8000 time: 0.1310s\n",
            "Epoch: 0151 loss_train: 0.4588 acc_train: 0.8429 loss_val: 0.6716 acc_val: 0.8000 time: 0.1312s\n",
            "Epoch: 0152 loss_train: 0.3936 acc_train: 0.8571 loss_val: 0.6723 acc_val: 0.8000 time: 0.1310s\n",
            "Epoch: 0153 loss_train: 0.5071 acc_train: 0.8357 loss_val: 0.6731 acc_val: 0.8000 time: 0.1311s\n",
            "Epoch: 0154 loss_train: 0.3983 acc_train: 0.8714 loss_val: 0.6741 acc_val: 0.7980 time: 0.1311s\n",
            "Epoch: 0155 loss_train: 0.3479 acc_train: 0.9071 loss_val: 0.6750 acc_val: 0.7980 time: 0.1312s\n",
            "Epoch: 0156 loss_train: 0.3606 acc_train: 0.8857 loss_val: 0.6753 acc_val: 0.7980 time: 0.1310s\n",
            "Epoch: 0157 loss_train: 0.3938 acc_train: 0.8571 loss_val: 0.6754 acc_val: 0.7980 time: 0.1313s\n",
            "Epoch: 0158 loss_train: 0.3289 acc_train: 0.8929 loss_val: 0.6760 acc_val: 0.7960 time: 0.1315s\n",
            "Epoch: 0159 loss_train: 0.5116 acc_train: 0.8143 loss_val: 0.6760 acc_val: 0.7960 time: 0.1317s\n",
            "Epoch: 0160 loss_train: 0.4508 acc_train: 0.8357 loss_val: 0.6757 acc_val: 0.7980 time: 0.1313s\n",
            "Epoch: 0161 loss_train: 0.4393 acc_train: 0.8357 loss_val: 0.6759 acc_val: 0.7980 time: 0.1316s\n",
            "Epoch: 0162 loss_train: 0.2922 acc_train: 0.9000 loss_val: 0.6762 acc_val: 0.7980 time: 0.1317s\n",
            "Epoch: 0163 loss_train: 0.3526 acc_train: 0.8714 loss_val: 0.6759 acc_val: 0.7980 time: 0.1312s\n",
            "Epoch: 0164 loss_train: 0.5115 acc_train: 0.8357 loss_val: 0.6757 acc_val: 0.7960 time: 0.1310s\n",
            "Epoch: 0165 loss_train: 0.4423 acc_train: 0.8500 loss_val: 0.6758 acc_val: 0.7960 time: 0.1306s\n",
            "Epoch: 0166 loss_train: 0.4296 acc_train: 0.8786 loss_val: 0.6755 acc_val: 0.7980 time: 0.1309s\n",
            "Epoch: 0167 loss_train: 0.4746 acc_train: 0.8214 loss_val: 0.6750 acc_val: 0.8000 time: 0.1311s\n",
            "Epoch: 0168 loss_train: 0.4171 acc_train: 0.8643 loss_val: 0.6743 acc_val: 0.8000 time: 0.1317s\n",
            "Epoch: 0169 loss_train: 0.4361 acc_train: 0.8786 loss_val: 0.6732 acc_val: 0.7980 time: 0.1311s\n",
            "Epoch: 0170 loss_train: 0.4122 acc_train: 0.8429 loss_val: 0.6724 acc_val: 0.8020 time: 0.1316s\n",
            "Epoch: 0171 loss_train: 0.3631 acc_train: 0.8857 loss_val: 0.6715 acc_val: 0.8020 time: 0.1308s\n",
            "Epoch: 0172 loss_train: 0.3191 acc_train: 0.8857 loss_val: 0.6709 acc_val: 0.8040 time: 0.1309s\n",
            "Epoch: 0173 loss_train: 0.4640 acc_train: 0.8286 loss_val: 0.6711 acc_val: 0.8040 time: 0.1311s\n",
            "Epoch: 0174 loss_train: 0.5028 acc_train: 0.7857 loss_val: 0.6716 acc_val: 0.8060 time: 0.1310s\n",
            "Epoch: 0175 loss_train: 0.3126 acc_train: 0.8929 loss_val: 0.6732 acc_val: 0.8020 time: 0.1337s\n",
            "Epoch: 0176 loss_train: 0.4562 acc_train: 0.8429 loss_val: 0.6743 acc_val: 0.7980 time: 0.1310s\n",
            "Epoch: 0177 loss_train: 0.3690 acc_train: 0.8786 loss_val: 0.6753 acc_val: 0.7980 time: 0.1314s\n",
            "Epoch: 0178 loss_train: 0.3591 acc_train: 0.8714 loss_val: 0.6767 acc_val: 0.7980 time: 0.1308s\n",
            "Epoch: 0179 loss_train: 0.4159 acc_train: 0.8786 loss_val: 0.6784 acc_val: 0.7980 time: 0.1312s\n",
            "Epoch: 0180 loss_train: 0.4580 acc_train: 0.8286 loss_val: 0.6816 acc_val: 0.7960 time: 0.1315s\n",
            "Epoch: 0181 loss_train: 0.4014 acc_train: 0.8500 loss_val: 0.6844 acc_val: 0.7960 time: 0.1310s\n",
            "Epoch: 0182 loss_train: 0.4362 acc_train: 0.8429 loss_val: 0.6869 acc_val: 0.7940 time: 0.1355s\n",
            "Epoch: 0183 loss_train: 0.4918 acc_train: 0.7929 loss_val: 0.6895 acc_val: 0.7960 time: 0.1313s\n",
            "Epoch: 0184 loss_train: 0.3774 acc_train: 0.8786 loss_val: 0.6924 acc_val: 0.7940 time: 0.1311s\n",
            "Epoch: 0185 loss_train: 0.4153 acc_train: 0.8571 loss_val: 0.6955 acc_val: 0.7940 time: 0.1317s\n",
            "Epoch: 0186 loss_train: 0.3552 acc_train: 0.8857 loss_val: 0.6985 acc_val: 0.7940 time: 0.1313s\n",
            "Epoch: 0187 loss_train: 0.4439 acc_train: 0.8429 loss_val: 0.7011 acc_val: 0.7940 time: 0.1310s\n",
            "Epoch: 0188 loss_train: 0.2839 acc_train: 0.9286 loss_val: 0.7036 acc_val: 0.7920 time: 0.1310s\n",
            "Epoch: 0189 loss_train: 0.3075 acc_train: 0.8929 loss_val: 0.7058 acc_val: 0.7900 time: 0.1309s\n",
            "Epoch: 0190 loss_train: 0.4615 acc_train: 0.8429 loss_val: 0.7080 acc_val: 0.7900 time: 0.1311s\n",
            "Epoch: 0191 loss_train: 0.4197 acc_train: 0.8714 loss_val: 0.7101 acc_val: 0.7880 time: 0.1307s\n",
            "Epoch: 0192 loss_train: 0.4860 acc_train: 0.8357 loss_val: 0.7109 acc_val: 0.7880 time: 0.1310s\n",
            "Epoch: 0193 loss_train: 0.3702 acc_train: 0.8714 loss_val: 0.7109 acc_val: 0.7900 time: 0.1312s\n",
            "Epoch: 0194 loss_train: 0.3700 acc_train: 0.8714 loss_val: 0.7105 acc_val: 0.7900 time: 0.1311s\n",
            "Epoch: 0195 loss_train: 0.5073 acc_train: 0.8214 loss_val: 0.7097 acc_val: 0.7920 time: 0.1311s\n",
            "Epoch: 0196 loss_train: 0.3947 acc_train: 0.8714 loss_val: 0.7080 acc_val: 0.7920 time: 0.1317s\n",
            "Epoch: 0197 loss_train: 0.4158 acc_train: 0.8500 loss_val: 0.7056 acc_val: 0.7940 time: 0.1308s\n",
            "Epoch: 0198 loss_train: 0.4118 acc_train: 0.8429 loss_val: 0.7037 acc_val: 0.7960 time: 0.1316s\n",
            "Epoch: 0199 loss_train: 0.3982 acc_train: 0.8286 loss_val: 0.7016 acc_val: 0.7960 time: 0.1313s\n",
            "Epoch: 0200 loss_train: 0.4173 acc_train: 0.8571 loss_val: 0.6997 acc_val: 0.7940 time: 0.1312s\n",
            "Optimization Finished!\n",
            "Total time elapsed: 28.0523s\n",
            "Loading 136th epoch\n",
            "Test set results: loss= 0.6234 accuracy= 0.8040\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YwI0i7xoNl4n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eQQ9A9fuF78Y"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}